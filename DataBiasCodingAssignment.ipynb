{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3433d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58211570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"attributeScores\": {\n",
      "    \"TOXICITY\": {\n",
      "      \"spanScores\": [\n",
      "        {\n",
      "          \"begin\": 0,\n",
      "          \"end\": 30,\n",
      "          \"score\": {\n",
      "            \"value\": 0.02543884,\n",
      "            \"type\": \"PROBABILITY\"\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"summaryScore\": {\n",
      "        \"value\": 0.02543884,\n",
      "        \"type\": \"PROBABILITY\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"languages\": [\n",
      "    \"en\"\n",
      "  ],\n",
      "  \"detectedLanguages\": [\n",
      "    \"en\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient import discovery\n",
    "import json\n",
    "\n",
    "API_KEY = 'api-key'\n",
    "\n",
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n",
    "\n",
    "analyze_request = {\n",
    "  'comment': { 'text': 'friendly greetings from python' },\n",
    "  'requestedAttributes': {'TOXICITY': {}}\n",
    "}\n",
    "\n",
    "response = client.comments().analyze(body=analyze_request).execute()\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0be6594",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = pd.read_csv(\"test_comments.csv\")\n",
    "\n",
    "\n",
    "test_comments = test_file['comments']\n",
    "str_test_comments = []\n",
    "\n",
    "test_comments_class = test_file['toxic']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f99524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_short_strings_scores = []\n",
    "predicted_long_strings_scores = []\n",
    "\n",
    "actual_short_strings_scores = []\n",
    "actual_long_strings_scores = []\n",
    "\n",
    "str_test_comments = []\n",
    "for i in test_comments:\n",
    "    i = str(i)\n",
    "    str_test_comments.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe833ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1]\n",
      "43\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "actual_class_list = []\n",
    "\n",
    "for i in test_comments_class:\n",
    "    i = str(i)\n",
    "    if i == \"no\":\n",
    "        actual_class_list.append(0)\n",
    "    else:\n",
    "        actual_class_list.append(1)\n",
    "        \n",
    "print(actual_class_list)\n",
    "\n",
    "actual_short_toxicity_scores = []\n",
    "actual_long_toxicity_scores = []\n",
    "    \n",
    "linked = list(zip(str_test_comments,actual_class_list))\n",
    "\n",
    "long_strings = []\n",
    "short_strings = []\n",
    "\n",
    "for pair in linked:\n",
    "    if len(pair[0]) > 250:\n",
    "        long_strings.append(pair[0])\n",
    "        actual_long_toxicity_scores.append(pair[1])\n",
    "    else:\n",
    "        short_strings.append(pair[0])\n",
    "        actual_short_toxicity_scores.append(pair[1])\n",
    "#print(short_strings)\n",
    "print(actual_short_toxicity_scores)\n",
    "print(len(actual_short_toxicity_scores))\n",
    "print(actual_long_toxicity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "646d994f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.027088705, 0.6852916, 0.06263174, 0.20009702, 0.24282593, 0.11223003, 0.050821137, 0.038048524, 0.023199737, 0.3561489, 0.20195828, 0.5885171, 0.050821137, 0.015393426, 0.24603334, 0.038991302, 0.43230394, 0.34328604, 0.78855824, 0.034277402, 0.017089844, 0.44393396, 0.014011159, 0.027324399, 0.13214645, 0.85333383]\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "long_toxicity_scores = []\n",
    "for i in long_strings:\n",
    "    try:\n",
    "        analyze_request = {'comment': {'text': i} , 'requestedAttributes': {'TOXICITY': {}}}\n",
    "        response = client.comments().analyze(body=analyze_request).execute()\n",
    "        toxicity_score = response[\"attributeScores\"]['TOXICITY']['summaryScore']['value']\n",
    "        long_toxicity_scores.append(toxicity_score)\n",
    "    except:\n",
    "        pass\n",
    "print(long_toxicity_scores)\n",
    "print(len(long_toxicity_scores))\n",
    "\n",
    "long_scores_copy = long_toxicity_scores.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8440064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "[0.027442247, 0.011686437, 0.103928015, 0.039935954, 0.3972142, 0.6426206, 0.718943, 0.718943, 0.7510937, 0.4269174, 0.8696708, 0.049584184, 0.07212844, 0.09611836, 0.038048524, 0.8115627, 0.10089093, 0.015707577, 0.4014846, 0.6491204, 0.687436, 0.8696708, 0.5721988, 0.5721988, 0.9248995, 0.19314334, 0.47323486, 0.8778702, 0.8778702, 0.3389984, 0.01426248, 'error', 0.10522962, 0.017089844, 0.8988238, 0.016210219, 0.11140333, 0.028385026, 0.51980776, 0.025910228, 0.85850734, 0.45703048, 0.6544696]\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "actual_filtered_short_scores = []\n",
    "short_toxicity_scores = []\n",
    "count = 0\n",
    "index_remove = []\n",
    "for i in range(len(short_strings)):\n",
    "    try:\n",
    "        analyze_request = {'comment': {'text': short_strings[i]} , 'requestedAttributes': {'TOXICITY': {}}}\n",
    "        response = client.comments().analyze(body=analyze_request).execute()\n",
    "        toxicity_score = response[\"attributeScores\"]['TOXICITY']['summaryScore']['value']\n",
    "        short_toxicity_scores.append(toxicity_score)   \n",
    "    except:\n",
    "        short_toxicity_scores.append(\"error\")\n",
    "        print(i)\n",
    "        index_remove.append(i)\n",
    "        pass\n",
    "print(short_toxicity_scores)\n",
    "print(len(short_toxicity_scores))\n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab285567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.027442247, 0.011686437, 0.103928015, 0.039935954, 0.3972142, 0.6426206, 0.718943, 0.718943, 0.7510937, 0.4269174, 0.8696708, 0.049584184, 0.07212844, 0.09611836, 0.038048524, 0.8115627, 0.10089093, 0.015707577, 0.4014846, 0.6491204, 0.687436, 0.8696708, 0.5721988, 0.5721988, 0.9248995, 0.19314334, 0.47323486, 0.8778702, 0.8778702, 0.3389984, 0.01426248, 'error', 0.10522962, 0.017089844, 0.8988238, 0.016210219, 0.11140333, 0.028385026, 0.51980776, 0.025910228, 0.85850734, 0.45703048, 0.6544696]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "short_scores_copy = short_toxicity_scores.copy()  # Make a copy of the original list\n",
    "print(short_scores_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e25255f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31]\n"
     ]
    }
   ],
   "source": [
    "index_remove.sort(reverse=True)\n",
    "print(index_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80f03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#print(scores_copy)\n",
    "for i in index_remove:\n",
    "    try:\n",
    "        scores_copy.pop(i)\n",
    "    #actual_short_toxicity_scores\n",
    "    except IndexError:\n",
    "        print(i)\n",
    "print(len(actual_short_toxicity_scores))\n",
    "print(len(scores_copy))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "572251c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.027442247, 0), (0.011686437, 0), (0.103928015, 0), (0.039935954, 0), (0.3972142, 0), (0.6426206, 1), (0.718943, 1), (0.718943, 1), (0.7510937, 1), (0.4269174, 0), (0.8696708, 1), (0.049584184, 0), (0.07212844, 0), (0.09611836, 0), (0.038048524, 0), (0.8115627, 1), (0.10089093, 0), (0.015707577, 0), (0.4014846, 0), (0.6491204, 1), (0.687436, 1), (0.8696708, 1), (0.5721988, 1), (0.5721988, 1), (0.9248995, 0), (0.19314334, 0), (0.47323486, 1), (0.8778702, 1), (0.8778702, 1), (0.3389984, 0), (0.01426248, 0), ('error', 1), (0.10522962, 0), (0.017089844, 0), (0.8988238, 1), (0.016210219, 0), (0.11140333, 0), (0.028385026, 0), (0.51980776, 0), (0.025910228, 0), (0.85850734, 1), (0.45703048, 1), (0.6544696, 1)]\n"
     ]
    }
   ],
   "source": [
    "short_linked = list(zip(short_scores_copy, actual_short_toxicity_scores))\n",
    "print(short_linked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4335ee52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.027088705, 0), (0.6852916, 1), (0.06263174, 0), (0.20009702, 0), (0.24282593, 0), (0.11223003, 0), (0.050821137, 0), (0.038048524, 0), (0.023199737, 0), (0.3561489, 1), (0.20195828, 0), (0.5885171, 1), (0.050821137, 0), (0.015393426, 0), (0.24603334, 0), (0.038991302, 0), (0.43230394, 0), (0.34328604, 0), (0.78855824, 1), (0.034277402, 0), (0.017089844, 0), (0.44393396, 1), (0.014011159, 0), (0.027324399, 0), (0.13214645, 0), (0.85333383, 1)]\n"
     ]
    }
   ],
   "source": [
    "long_scores_copy = long_toxicity_scores.copy()\n",
    "long_linked = list(zip(long_scores_copy, actual_long_toxicity_scores))\n",
    "print(long_linked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a127f50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.027442247, 0), (0.011686437, 0), (0.103928015, 0), (0.039935954, 0), (0.3972142, 0), (0.6426206, 1), (0.718943, 1), (0.718943, 1), (0.7510937, 1), (0.4269174, 0), (0.8696708, 1), (0.049584184, 0), (0.07212844, 0), (0.09611836, 0), (0.038048524, 0), (0.8115627, 1), (0.10089093, 0), (0.015707577, 0), (0.4014846, 0), (0.6491204, 1), (0.687436, 1), (0.8696708, 1), (0.5721988, 1), (0.5721988, 1), (0.9248995, 0), (0.19314334, 0), (0.47323486, 1), (0.8778702, 1), (0.8778702, 1), (0.3389984, 0), (0.01426248, 0), (0.10522962, 0), (0.017089844, 0), (0.8988238, 1), (0.016210219, 0), (0.11140333, 0), (0.028385026, 0), (0.51980776, 0), (0.025910228, 0), (0.85850734, 1), (0.45703048, 1), (0.6544696, 1)]\n"
     ]
    }
   ],
   "source": [
    "error = 'error'\n",
    "filtered_short = []\n",
    "for pair in short_linked:\n",
    "    if pair[0] != 'error':\n",
    "        filtered_short.append(pair)\n",
    "    else:\n",
    "        pass\n",
    "print(filtered_short)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61065345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.027088705, 0), (0.6852916, 1), (0.06263174, 0), (0.20009702, 0), (0.24282593, 0), (0.11223003, 0), (0.050821137, 0), (0.038048524, 0), (0.023199737, 0), (0.3561489, 1), (0.20195828, 0), (0.5885171, 1), (0.050821137, 0), (0.015393426, 0), (0.24603334, 0), (0.038991302, 0), (0.43230394, 0), (0.34328604, 0), (0.78855824, 1), (0.034277402, 0), (0.017089844, 0), (0.44393396, 1), (0.014011159, 0), (0.027324399, 0), (0.13214645, 0), (0.85333383, 1)]\n"
     ]
    }
   ],
   "source": [
    "error = 'error'\n",
    "filtered_long = []\n",
    "for pair in long_linked:\n",
    "    if pair[0] != 'error':\n",
    "        filtered_long.append(pair)\n",
    "    else:\n",
    "        pass\n",
    "print(filtered_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f2f1aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8421052631578947\n",
      "0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "#toxic accuracy for short comments\n",
    "#non toxic accuracy for short comments\n",
    "STN = 0\n",
    "STP = 0\n",
    "SFN = 0\n",
    "SFP = 0\n",
    "for pair in filtered_short:\n",
    "    if pair[0] >= 0.5:\n",
    "        if pair[1] == 1:\n",
    "            STP+=1 #count of true positive\n",
    "        else:\n",
    "            SFP+=1\n",
    "    elif pair[0] < 0.5:\n",
    "        if pair[1] == 0:\n",
    "            STN+=1 #count of true negative\n",
    "        else:\n",
    "            SFN+=1 \n",
    "\n",
    "short_toxic_acc = STP/actual_short_toxicity_scores.count(1)\n",
    "print(short_toxic_acc)\n",
    "\n",
    "short_nontoxic_acc = STN/actual_short_toxicity_scores.count(0)\n",
    "print(short_nontoxic_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42c75ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#toxic accuracy for long comments\n",
    "#non toxic accuracy for long comments\n",
    "\n",
    "LTN = 0\n",
    "LTP = 0\n",
    "LFN = 0\n",
    "LFP = 0\n",
    "for pair in filtered_long:\n",
    "    if pair[0] >= 0.5:\n",
    "        if pair[1] == 1:\n",
    "            LTP+=1 #count of true positive\n",
    "        else:\n",
    "            LFP+=1\n",
    "    elif pair[0] < 0.5:\n",
    "        if pair[1] == 0:\n",
    "            LTN+=1 #count of true negative\n",
    "        else:\n",
    "            LFN+=1 \n",
    "\n",
    "long_toxic_acc = LTP/actual_long_toxicity_scores.count(1)\n",
    "print(long_toxic_acc)\n",
    "\n",
    "long_nontoxic_acc = LTN/actual_long_toxicity_scores.count(0)\n",
    "print(long_nontoxic_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a029bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1(toxic) accuracy for short comments is 0.8421052631578947\n",
      "Class 0(nontoxic) accuracy for short comments is 0.9166666666666666\n",
      "Class 1(toxic) accuracy for long comments is 0.6666666666666666\n",
      "Class 0(nontoxic) accuracy for long comments is 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Class 1(toxic) accuracy for short comments is\", short_toxic_acc)\n",
    "print(\"Class 0(nontoxic) accuracy for short comments is\", short_nontoxic_acc)\n",
    "print(\"Class 1(toxic) accuracy for long comments is\", long_toxic_acc)\n",
    "print(\"Class 0(nontoxic) accuracy for long comments is\", long_nontoxic_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
